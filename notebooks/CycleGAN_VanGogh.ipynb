{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 75676,
     "sourceType": "datasetVersion",
     "datasetId": 42780
    },
    {
     "sourceId": 3956508,
     "sourceType": "datasetVersion",
     "datasetId": 850761
    }
   ],
   "dockerImageVersionId": 30035,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## Imports",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from random import shuffle\n",
    "from glob import glob\n",
    "import warnings\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Checking for GPU\n",
    "Using GPU is highly recommended for training GANs. It speeds up the training process significantly."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tf.config.list_physical_devices()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "device = '/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'\n",
    "print(device)\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading Datasets\n",
    "Check `sources.md` file to get the sources of datasets and learn more about them."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "main_path = \"../dataset/VincentVanGogh\"\n",
    "van_gogh_paths = []\n",
    "\n",
    "img_paths = glob(main_path + \"/*\")\n",
    "for img_path in img_paths:\n",
    "    van_gogh_paths.append(img_path)\n",
    "\n",
    "print(f\"Style images amount (Van Gogh Paintings): {len(van_gogh_paths)}\")"
   ],
   "metadata": {
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "main_path = \"../dataset/natural_images\"\n",
    "natural_img_paths = []\n",
    "\n",
    "class_paths = [os.path.join(main_path, class_name) for class_name in os.listdir(main_path)]\n",
    "for path in class_paths:\n",
    "    img_paths = glob(path + \"/*\")\n",
    "    for img_path in img_paths:\n",
    "        natural_img_paths.append(img_path)\n",
    "\n",
    "print(f\"Normal images amount (Natural images): {len(natural_img_paths)}\")"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reading images\n",
    "Reading the images based on paths defined above."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "style_images = []\n",
    "\n",
    "for i, style_path in enumerate(van_gogh_paths):\n",
    "    img = cv2.imread(style_path)\n",
    "    img = cv2.resize(img, (256, 256))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    style_images.append(img)\n",
    "    if not (i + 1) % 200:\n",
    "        print(f\"Current style image: #{i + 1} ({len(van_gogh_paths) - i + 1} left)\")\n",
    "print(f\"\\nLoaded {len(style_images)} style images!\\n\")"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "normal_images = []\n",
    "\n",
    "for i, normal_path in enumerate(natural_img_paths):\n",
    "    img = cv2.imread(normal_path)\n",
    "    img = cv2.resize(img, (256, 256))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    normal_images.append(img)\n",
    "    if not (i + 1) % 500:\n",
    "        print(f\"Current normal image: #{i + 1} ({len(natural_img_paths) - i + 1} left)\")\n",
    "print(f\"\\nLoaded {len(normal_images)} normal images!\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Processing the images"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def preprocess_image(images: list, low_memory: bool = False, print_shape: bool = True):\n",
    "    if low_memory and len(images) > 100:  # Dropping half of the images to reduce memory usage\n",
    "        shuffle(images)\n",
    "        images = images[:len(images) // 2]\n",
    "\n",
    "    images = np.array(images, dtype=np.float32)  # Converting to float32\n",
    "    images = images / 255  # Scaling between 0 and 1\n",
    "    \n",
    "    if print_shape:\n",
    "        print(\"Shape: \", images.shape)\n",
    "    return tf.data.Dataset.from_tensor_slices(images).batch(1)\n",
    "\n",
    "\n",
    "style_images = preprocess_image(style_images)\n",
    "normal_images = preprocess_image(normal_images, low_memory=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Visualizing the datasets",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(7, 7))\n",
    "plt.title(\"Style Images (Van Gogh)\")\n",
    "for i, image in enumerate(style_images.shuffle(10000).take(16)):\n",
    "    ax = plt.subplot(4, 4, i + 1)\n",
    "    ax.imshow(image[0])\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(7, 7))\n",
    "plt.title(\"Normal Images (Natural Images)\")\n",
    "for i, image in enumerate(normal_images.shuffle(10000).take(16)):\n",
    "    ax = plt.subplot(4, 4, i + 1)\n",
    "    ax.imshow(image[0])\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Instance Normalization layer rewrite\n",
    "Original source used InstanceNormalization layer from `tensorflow_addons` which have recently reached their end of life. In order to use the same functionality, I've rewritten the layer in `tf.keras`."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class InstanceNormalization(layers.Layer):\n",
    "    def __init__(self, epsilon: float = 1e-5, gamma_initializer=\"ones\", beta_initializer=\"zeros\", **kwargs):\n",
    "        super(InstanceNormalization, self).__init__(**kwargs)\n",
    "        self.beta, self.gamma = None, None\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma_initializer = gamma_initializer\n",
    "        self.beta_initializer = beta_initializer\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.gamma = self.add_weight(\n",
    "            shape=(input_shape[-1],),\n",
    "            initializer=self.gamma_initializer,\n",
    "            trainable=True,\n",
    "            name=\"gamma\"\n",
    "        )\n",
    "        self.beta = self.add_weight(\n",
    "            shape=(input_shape[-1],),\n",
    "            initializer=self.beta_initializer,\n",
    "            trainable=True,\n",
    "            name=\"beta\"\n",
    "        )\n",
    "        super(InstanceNormalization, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        mean, variance = tf.nn.moments(inputs, axes=[1, 2], keepdims=True)\n",
    "        normalized = (inputs - mean) / tf.sqrt(variance + self.epsilon)\n",
    "        return self.gamma * normalized + self.beta\n",
    "\n",
    "    def get_config(self) -> dict:\n",
    "        config = super(InstanceNormalization, self).get_config()\n",
    "        config.update({\"epsilon\": self.epsilon})\n",
    "        return config"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Upsample and Downsample Layers\n",
    "Model bases on downsampling and upsampling the images. For this exact purpose, we need to define these layers below."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "output_channels = len([\"Red\", \"Green\", \"Blue\"])  # RGB\n",
    "\n",
    "def downsample(filters: int, size: int, instance_norm: bool = True):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    gamma_init = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "    \n",
    "    result = keras.Sequential()\n",
    "    result.add(layers.Conv2D(filters, size, strides=2, padding=\"same\", kernel_initializer=initializer, use_bias=False))\n",
    "    if instance_norm:\n",
    "        result.add(InstanceNormalization(gamma_initializer=gamma_init))\n",
    "    result.add(layers.LeakyReLU())\n",
    "    return result\n",
    "\n",
    "\n",
    "def upsample(filters: int, size: int, dropout: bool = False):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    gamma_init = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "\n",
    "    result = keras.Sequential()\n",
    "    result.add(layers.Conv2DTranspose(filters, size, strides=2,\n",
    "                                      padding=\"same\", kernel_initializer=initializer, use_bias=False))\n",
    "    result.add(InstanceNormalization(gamma_initializer=gamma_init))\n",
    "    if dropout:\n",
    "        result.add(layers.Dropout(0.5))\n",
    "    result.add(layers.ReLU())\n",
    "    return result"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generator Model initialization\n",
    "Here's a quick recap of how the generator works:\n",
    "1. Generator takes 256x256x3 image and converts into 1x1x512 vector\n",
    "2. That vector is upsampled to 256x256x3\n",
    "3. To prevent vanishing gradient problem, skip connections are created and the model is returned"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class Generator:\n",
    "    def __init__(self):\n",
    "        self.initializer = tf.random_normal_initializer(0., 0.02)\n",
    "        self.inputs = layers.Input([256, 256, 3])\n",
    "        self.outputs = None\n",
    "        self.model = None\n",
    "        self.down_stack, self.up_stack = None, None\n",
    "        self.prepare_stacks()\n",
    "\n",
    "    def __call__(self, inputs, training: bool = True):\n",
    "        if not self.model:\n",
    "            self.build()\n",
    "        return self.model(inputs, training=training)\n",
    "\n",
    "    def prepare_stacks(self):\n",
    "        self.down_stack = [downsample(64, 4),  # 128x128x64\n",
    "                           downsample(128, 4),  # 64x64x128\n",
    "                           downsample(256, 4),  # 32x32x256\n",
    "                           downsample(512, 4),  # 16x16x512\n",
    "                           downsample(512, 4),  # 8x8x512\n",
    "                           downsample(512, 4),  # 4x4x512\n",
    "                           downsample(512, 4),  # 2x2x512\n",
    "                           downsample(512, 4),  # 1x1x512\n",
    "                           ]\n",
    "        self.up_stack = [upsample(512, 4, dropout=True),  # 2x2\n",
    "                         upsample(512, 4, dropout=True),  # 4x4\n",
    "                         upsample(512, 4),  # 8x8\n",
    "                         upsample(256, 4),  # 16x16\n",
    "                         upsample(128, 4),  # 32x32\n",
    "                         upsample(64, 4),  # 64x64\n",
    "                         upsample(32, 4),  # 128x128\n",
    "                         ]\n",
    "\n",
    "    def build(self) -> keras.Model:\n",
    "        last_layer = layers.Conv2DTranspose(output_channels, 4,\n",
    "                                            strides=2, padding='same',\n",
    "                                            kernel_initializer=self.initializer,\n",
    "                                            activation='tanh')\n",
    "\n",
    "        x = self.inputs\n",
    "        skips = []\n",
    "        for down in self.down_stack:\n",
    "            x = down(x)\n",
    "            skips.append(x)\n",
    "        skips = reversed(skips[:-1])\n",
    "\n",
    "        for up, skip in zip(self.up_stack, skips):\n",
    "            x = up(x)\n",
    "            x = layers.Concatenate()([x, skip])\n",
    "        self.outputs = last_layer(x)\n",
    "        self.model = keras.Model(inputs=self.inputs, outputs=self.outputs)\n",
    "        return self.model"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Discriminator Model initialization\n",
    "In this section I am going to build the discriminator model. This Discriminator is a CNN based classifier. Its main purpose is to determine whether given image is real or generated."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class Discriminator:\n",
    "    def __init__(self):\n",
    "        self.initializer = tf.random_normal_initializer(0., 0.02)\n",
    "        self.gamma_init = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "        self.inputs = layers.Input([256, 256, 3], name=\"input_image\")\n",
    "        self.outputs = None\n",
    "        self.model = None\n",
    "    \n",
    "    def build(self) -> keras.Model:\n",
    "        down_layer1 = downsample(32, 4, False)(self.inputs)   # 128x128x32\n",
    "        down_layer2 = downsample(64, 4, False)(down_layer1)   # 64x64x64\n",
    "        down_layer3 = downsample(128, 4, False)(down_layer2)  # 32x32x128\n",
    "        \n",
    "        zero_pad_layer1 = layers.ZeroPadding2D()(down_layer3)\n",
    "        conv_layer = layers.Conv2D(512, 4, strides=1,\n",
    "                             kernel_initializer=self.initializer,\n",
    "                             use_bias=False)(zero_pad_layer1)\n",
    "        \n",
    "        norm_layer = InstanceNormalization(gamma_initializer=self.gamma_init)(conv_layer)\n",
    "        leaky_relu_layer = layers.LeakyReLU()(norm_layer)\n",
    "        zero_pad_layer2 = layers.ZeroPadding2D()(leaky_relu_layer)\n",
    "        \n",
    "        self.outputs = layers.Conv2D(1, 4, strides=1, kernel_initializer=self.initializer)(zero_pad_layer2)\n",
    "        self.model = keras.Model(inputs=self.inputs, outputs=self.outputs)\n",
    "        return self.model"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Generators and Discriminators initialization  ",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "style_generator = Generator().build() # generates styled paintings using natural images\n",
    "photo_generator = Generator().build() # generates natural images using styled paintings\n",
    "\n",
    "style_discriminator = Discriminator().build() # determines whether generated styled painting is real or generated\n",
    "photo_discriminator = Discriminator().build() # determines whether natural image is real or generated"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Visualizing the models\n",
    "Requires `Graphviz` to be installed!\n",
    "> If you want to export the models to _.png_, you can uncomment the second line in cells below."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "keras.utils.plot_model(style_generator, show_shapes=True, show_layer_names=True)\n",
    "# keras.utils.plot_model(style_generator, to_file=\"../docs/imgs/cyclegan_generator.png\", show_shapes=True, show_layer_names=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "keras.utils.plot_model(style_discriminator, show_shapes=True, show_layer_names=True)\n",
    "# keras.utils.plot_model(style_discriminator, to_file=\"../docs/imgs/cyclegan_discriminator.png\", show_shapes=True, show_layer_names=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Building CycleGAN Model\n",
    "Here we define the structure of CycleGAN model. Training process will be as follows:\n",
    "1. Creating fake photos, same photos and cycled photos that will be used in order to compute loss\n",
    "2. Discriminators decide whether the generated images are real or fake\n",
    "3. Based on computed losses, gradients are calculated and applied to networks"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class CycleGAN(keras.Model):\n",
    "    def __init__(self, style_gen: keras.Model, photo_gen: keras.Model,\n",
    "                 style_disc: keras.Model, photo_disc: keras.Model, lambda_cycle: int = 10):\n",
    "        super(CycleGAN, self).__init__()\n",
    "        self.style_gen = style_gen\n",
    "        self.style_disc = style_disc\n",
    "        self.photo_gen = photo_gen\n",
    "        self.photo_disc = photo_disc\n",
    "        self.lambda_cycle = lambda_cycle\n",
    "        self.photo_gen_optimizer, self.photo_disc_optimizer = None, None\n",
    "        self.style_gen_optimizer, self.style_disc_optimizer = None, None\n",
    "        self.identity_loss_fn, self.cycle_loss_fn = None, None\n",
    "        self.disc_loss_fn, self.gen_loss_fn = None, None\n",
    "\n",
    "    def compile(self, style_gen_optimizer, photo_gen_optimizer, style_disc_optimizer, photo_disc_optimizer,\n",
    "                gen_loss_fn, disc_loss_fn, cycle_loss_fn, identity_loss_fn, **kwargs):\n",
    "        super(CycleGAN, self).compile(**kwargs)\n",
    "        self.style_gen_optimizer = style_gen_optimizer\n",
    "        self.photo_gen_optimizer = photo_gen_optimizer\n",
    "        self.style_disc_optimizer = style_disc_optimizer\n",
    "        self.photo_disc_optimizer = photo_disc_optimizer\n",
    "        self.gen_loss_fn = gen_loss_fn\n",
    "        self.disc_loss_fn = disc_loss_fn\n",
    "        self.cycle_loss_fn = cycle_loss_fn\n",
    "        self.identity_loss_fn = identity_loss_fn\n",
    "\n",
    "    def train_step(self, batch_data) -> dict:\n",
    "        real_style, real_photo = batch_data\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # Photo -> Styled -> Photo\n",
    "            fake_style = self.style_gen(real_photo, training=True)\n",
    "            cycled_photo = self.photo_gen(fake_style, training=True)\n",
    "\n",
    "            # Styled -> Photo -> Styled\n",
    "            fake_photo = self.photo_gen(real_style, training=True)\n",
    "            cycled_style = self.style_gen(fake_photo, training=True)\n",
    "\n",
    "            # Generator generates\n",
    "            same_photo = self.photo_gen(real_photo, training=True)\n",
    "            same_style = self.style_gen(real_style, training=True)\n",
    "\n",
    "            # Discriminator decided\n",
    "            disc_real_style = self.style_disc(real_style, training=True)\n",
    "            disc_real_photo = self.photo_disc(real_photo, training=True)\n",
    "            disc_fake_style = self.style_disc(fake_style, training=True)\n",
    "            disc_fake_photo = self.photo_disc(fake_photo, training=True)\n",
    "\n",
    "            # Generator loss\n",
    "            style_gen_loss = self.gen_loss_fn(disc_fake_style)\n",
    "            photo_gen_loss = self.gen_loss_fn(disc_fake_photo)\n",
    "\n",
    "            # Total cycle loss\n",
    "            total_cycle_loss = self.cycle_loss_fn(real_style, cycled_style, self.lambda_cycle) + self.cycle_loss_fn(real_photo, cycled_photo, self.lambda_cycle)\n",
    "\n",
    "            # Total loss\n",
    "            total_style_gen_loss = style_gen_loss + total_cycle_loss + self.identity_loss_fn(real_style, same_style, self.lambda_cycle)\n",
    "            total_photo_gen_loss = photo_gen_loss + total_cycle_loss + self.identity_loss_fn(real_photo, same_photo, self.lambda_cycle)\n",
    "\n",
    "            # Discriminator loss\n",
    "            style_disc_loss = self.disc_loss_fn(disc_real_style, disc_fake_style)\n",
    "            photo_disc_loss = self.disc_loss_fn(disc_real_photo, disc_fake_photo)\n",
    "\n",
    "        # Calculating gradients of networks\n",
    "        style_generator_gradients = tape.gradient(total_style_gen_loss, self.style_gen.trainable_variables)\n",
    "        photo_generator_gradients = tape.gradient(total_photo_gen_loss, self.photo_gen.trainable_variables)\n",
    "        style_discriminator_gradients = tape.gradient(style_disc_loss, self.style_disc.trainable_variables)\n",
    "        photo_discriminator_gradients = tape.gradient(photo_disc_loss, self.photo_disc.trainable_variables)\n",
    "\n",
    "        self.style_gen_optimizer.apply_gradients(zip(style_generator_gradients, self.style_gen.trainable_variables))\n",
    "        self.photo_gen_optimizer.apply_gradients(zip(photo_generator_gradients, self.photo_gen.trainable_variables))\n",
    "\n",
    "        self.style_disc_optimizer.apply_gradients(zip(style_discriminator_gradients,\n",
    "                                                      self.style_disc.trainable_variables))\n",
    "        self.photo_disc_optimizer.apply_gradients(zip(photo_discriminator_gradients,\n",
    "                                                      self.photo_disc.trainable_variables))\n",
    "\n",
    "        return {\n",
    "            \"style_gen_loss\": total_style_gen_loss,\n",
    "            \"photo_gen_loss\": total_photo_gen_loss,\n",
    "            \"style_disc_loss\": style_disc_loss,\n",
    "            \"photo_disc_loss\": photo_disc_loss\n",
    "        }"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Defining loss functions",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "\n",
    "def discriminator_loss(real, generated) -> tf.Tensor:  # avg of real and generated images loss\n",
    "    real_loss = cross_entropy(tf.ones_like(real), real)\n",
    "    generated_loss = cross_entropy(tf.zeros_like(generated), generated)\n",
    "    return (real_loss + generated_loss) / 2\n",
    "\n",
    "\n",
    "def generator_loss(generated) -> tf.Tensor:  # generated images loss\n",
    "    return cross_entropy(tf.ones_like(generated), generated)\n",
    "\n",
    "\n",
    "def cycle_loss(real_image, cycled_image, _lambda: float) -> tf.Tensor:  # generated images loss\n",
    "    return tf.reduce_mean(tf.abs(real_image - cycled_image)) * _lambda\n",
    "\n",
    "\n",
    "def identity_loss(real_image, same_image, _lambda: float) -> tf.Tensor:\n",
    "    return tf.reduce_mean(tf.abs(real_image - same_image)) * _lambda"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Defining optimizers\n",
    "Adam optimizer's adaptive learning rate, efficient handling of noisy and sparse gradients and ease of use make it a great choice for complex and dynamic training process in GANs."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "style_generator_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5)\n",
    "photo_generator_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5)\n",
    "\n",
    "style_discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5)\n",
    "photo_discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### CycleGAN initialization and compilation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "with tf.device(device):\n",
    "    model = CycleGAN(style_gen=style_generator, photo_gen=photo_generator,\n",
    "                    style_disc=style_discriminator, photo_disc=photo_discriminator)\n",
    "    \n",
    "    model.compile(style_gen_optimizer=style_generator_optimizer,\n",
    "                  photo_gen_optimizer=photo_generator_optimizer,\n",
    "                  photo_disc_optimizer=photo_discriminator_optimizer,\n",
    "                  style_disc_optimizer=style_discriminator_optimizer,\n",
    "                  gen_loss_fn=generator_loss, disc_loss_fn=discriminator_loss,\n",
    "                  cycle_loss_fn=cycle_loss, identity_loss_fn=identity_loss)"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training the CycleGAN model\n",
    "- loads the saved weights if available\n",
    "- trains the model for given amount of epochs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "epochs = 40\n",
    "\n",
    "with tf.device(device):\n",
    "    # if os.path.exists(\"../models/van_gogh_model.weights.h5\"):\n",
    "    #     model.load_weights(\"../models/van_gogh_model.weights.h5\")\n",
    "    #     print(\"Loaded .weights.h5 file!\")\n",
    "    model.fit(tf.data.Dataset.zip((style_images, normal_images)), epochs=epochs)\n",
    "    style_generator.save_weights(\"../models/van_gogh.weights.h5\")\n",
    "    model.save_weights(\"../models/van_gogh_model.weights.h5\")"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# style_generator.save_weights(\"../models/van_gogh.weights.h5\")  # in case saving above fails (example: lack of memory)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# model.save_weights(\"van_gogh.weights.h5\")  # in case saving above fails (example: lack of memory)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Sample painting generation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "plt.figure(dpi=500)\n",
    "for i, image in enumerate(normal_images.shuffle(10000).take(36)):\n",
    "    plt.subplot(6, 12, 2 * i + 1)\n",
    "    plt.imshow(image[0])\n",
    "    plt.axis(\"off\")\n",
    "    prediction = style_generator(image, training=False)[0].numpy()\n",
    "    prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n",
    "    plt.subplot(6, 12, 2 * (i + 1))\n",
    "    plt.imshow(prediction)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.savefig(\"../tmp/van_gogh_result.png\")\n",
    "plt.show()"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
