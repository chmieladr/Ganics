{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## Imports",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from random import shuffle\n",
    "from sys import stderr\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-05-22T12:12:45.161170Z",
     "iopub.execute_input": "2021-05-22T12:12:45.161534Z",
     "iopub.status.idle": "2021-05-22T12:12:50.095077Z",
     "shell.execute_reply.started": "2021-05-22T12:12:45.161459Z",
     "shell.execute_reply": "2021-05-22T12:12:50.094163Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tf.config.list_physical_devices()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "device = '/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'\n",
    "print(device)\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading Dataset\n",
    "* _Universal Image Embeddings (512x512)_ dataset with 130k+ images is used along with preprocessed both _Vincent Van Gogh art_ and _Cartoon - Family Guy_ images\n",
    "* only 256 random images are used for faster computation - can be changed with `data_size` parameter\n",
    "* `img256` -> _np.array_ with input images generated by resizing original images to **256x256** size (size of the _CycleGAN_ output)\n",
    "* `img512` -> _np.array_ with original images of **512x512** size"
   ],
   "metadata": {}
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "img256 = []\n",
    "img512 = []\n",
    "data_size = 256\n",
    "test_size = 16\n",
    "\n",
    "paths = []\n",
    "directories = [\"../dataset/Familyguy\", \"../dataset/VincentVanGogh_standardized\"]\n",
    "\n",
    "for dir_name in directories:\n",
    "    dir_paths = os.listdir(dir_name)\n",
    "    full_paths = [os.path.join(dir_name, path) for path in dir_paths]\n",
    "    paths += full_paths\n",
    "\n",
    "shuffle(paths)\n",
    "paths = paths[0:data_size + test_size]\n",
    "\n",
    "for it, filename in enumerate(tqdm(paths)):\n",
    "    img = Image.open(filename)\n",
    "    img512.append(np.array(img))\n",
    "    img = img.resize((256, 256))\n",
    "    img256.append(np.array(img))\n",
    "\n",
    "img256 = np.array(img256, dtype='float32')\n",
    "img512 = np.array(img512, dtype='float32')\n",
    "\n",
    "img256 = (img256 / 127.5) - 1\n",
    "img512 = (img512 / 127.5) - 1\n",
    "\n",
    "img256_train = img256[0:data_size].reshape(-1, 1, 256, 256, 3)\n",
    "img512_train = img512[0:data_size].reshape(-1, 1, 512, 512, 3)\n",
    "\n",
    "img256_test = img256[data_size:].reshape(-1, 1, 256, 256, 3)\n",
    "img512_test = img512[data_size:].reshape(-1, 1, 512, 512, 3)\n",
    "\n",
    "print(img256_train.shape)\n",
    "print(img512_train.shape)\n",
    "\n",
    "print(img256_test.shape)\n",
    "print(img512_test.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Instance Normalization layer initialization\n",
    "`tf.keras.layers.BatchNormalization` has been replaced with my custom `tf.keras.layers.InstanceNormalization` layer to keep the consistency with other GANs within this project."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class InstanceNormalization(layers.Layer):\n",
    "    def __init__(self, epsilon: float = 1e-5, gamma_initializer=\"ones\", beta_initializer=\"zeros\", **kwargs):\n",
    "        super(InstanceNormalization, self).__init__(**kwargs)\n",
    "        self.beta, self.gamma = None, None\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma_initializer = gamma_initializer\n",
    "        self.beta_initializer = beta_initializer\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.gamma = self.add_weight(\n",
    "            shape=(input_shape[-1],),\n",
    "            initializer=self.gamma_initializer,\n",
    "            trainable=True,\n",
    "            name=\"gamma\"\n",
    "        )\n",
    "        self.beta = self.add_weight(\n",
    "            shape=(input_shape[-1],),\n",
    "            initializer=self.beta_initializer,\n",
    "            trainable=True,\n",
    "            name=\"beta\"\n",
    "        )\n",
    "        super(InstanceNormalization, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        mean, variance = tf.nn.moments(inputs, axes=[1, 2], keepdims=True)\n",
    "        normalized = (inputs - mean) / tf.sqrt(variance + self.epsilon)\n",
    "        return self.gamma * normalized + self.beta\n",
    "\n",
    "    def get_config(self) -> dict:\n",
    "        config = super(InstanceNormalization, self).get_config()\n",
    "        config.update({\"epsilon\": self.epsilon})\n",
    "        return config"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Upsample and Downsample Layers\n",
    "Model bases on downsampling and upsampling the images. For this exact purpose, we need to define these layers below."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "output_channels = len([\"Red\", \"Green\", \"Blue\"])  # RGB\n",
    "\n",
    "def downsample(filters: int, size: int, instance_norm: bool = True):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    gamma_init = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "\n",
    "    result = keras.Sequential()\n",
    "    result.add(layers.Conv2D(filters, size, strides=2, padding=\"same\", kernel_initializer=initializer, use_bias=False))\n",
    "    if instance_norm:\n",
    "        result.add(InstanceNormalization(gamma_initializer=gamma_init))\n",
    "    result.add(layers.LeakyReLU())\n",
    "    return result\n",
    "\n",
    "\n",
    "def upsample(filters: int, size: int, dropout: bool = False):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    gamma_init = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "\n",
    "    result = keras.Sequential()\n",
    "    result.add(layers.Conv2DTranspose(filters, size, strides=2, padding=\"same\", kernel_initializer=initializer, use_bias=False))\n",
    "    result.add(InstanceNormalization(gamma_initializer=gamma_init))\n",
    "    if dropout:\n",
    "        result.add(layers.Dropout(0.5))\n",
    "    result.add(layers.ReLU())\n",
    "    return result"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-05-22T12:12:59.750428Z",
     "iopub.execute_input": "2021-05-22T12:12:59.750709Z",
     "iopub.status.idle": "2021-05-22T12:12:59.759095Z",
     "shell.execute_reply.started": "2021-05-22T12:12:59.750682Z",
     "shell.execute_reply": "2021-05-22T12:12:59.758114Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Generator model initialization\n",
    "Generator's goal is to generate photos that can be mistaken for real ones. In this case it's is a U-Net model consisting of three main components:\n",
    "* **Downsampling blocks** -> convert image input to tensors of lower dimensions until it becomes a 1D tensor\n",
    "* **Upsampling blocks** -> convert output of downsampling blocks back to image output\n",
    "* **Skip connections** -> provide connections between downsampling and upsampling blocks at each level\n",
    "Generator and Discriminator constantly compete against each other in order to improve the quality of generated images."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class Generator:\n",
    "    def __init__(self):\n",
    "        self.initializer = tf.random_normal_initializer(0., 0.02)\n",
    "        self.inputs = layers.Input([256, 256, 3])\n",
    "        self.outputs = None\n",
    "        self.model = None\n",
    "        self.down_stack, self.up_stack = None, None\n",
    "        self.prepare_stacks()\n",
    "\n",
    "    def __call__(self, inputs, training: bool = True):\n",
    "        if not self.model:\n",
    "            self.build()\n",
    "        return self.model(inputs, training=training)\n",
    "\n",
    "    def prepare_stacks(self):\n",
    "        # format: (bs = batch size, width, height, filters)\n",
    "        # each downsampling reduces size by 2 because of stride = 2,\n",
    "        self.down_stack = [\n",
    "            downsample(32, 4, instance_norm=False),  # (bs, 128, 128, 32)\n",
    "            downsample(64, 4),  # (bs, 64, 64, 64)\n",
    "            downsample(128, 4),  # (bs, 32, 32, 128)\n",
    "            downsample(256, 4),  # (bs, 16, 16, 256)\n",
    "            downsample(512, 4),  # (bs, 8, 8, 512)\n",
    "            downsample(512, 4),  # (bs, 4, 4, 512)\n",
    "            downsample(512, 4),  # (bs, 2, 2, 512)\n",
    "            downsample(512, 4),  # (bs, 1, 1, 512)\n",
    "        ]\n",
    "        self.up_stack = [\n",
    "            upsample(512, 4, dropout=True),  # (bs, 2, 2, 1024)\n",
    "            upsample(512, 4, dropout=True),  # (bs, 4, 4, 1024)\n",
    "            upsample(512, 4, dropout=True),  # (bs, 8, 8, 1024)\n",
    "            upsample(512, 4),  # (bs, 16, 16, 1024)\n",
    "            upsample(256, 4),  # (bs, 32, 32, 512)\n",
    "            upsample(128, 4),  # (bs, 64, 64, 256)\n",
    "            upsample(64, 4),  # (bs, 128, 128, 128)\n",
    "        ]\n",
    "\n",
    "    def build(self) -> keras.Model:\n",
    "        last_layer = layers.Conv2DTranspose(output_channels, 4, strides=4, padding='same',\n",
    "                                                     kernel_initializer=self.initializer,\n",
    "                                                     activation='tanh')  # (bs, 512, 512, 3)\n",
    "\n",
    "        x = self.inputs\n",
    "        skips = []\n",
    "        for down in self.down_stack:\n",
    "            x = down(x)\n",
    "            skips.append(x)\n",
    "        skips = reversed(skips[:-1])\n",
    "\n",
    "        for up, skip in zip(self.up_stack, skips):\n",
    "            x = up(x)\n",
    "            x = layers.Concatenate()([x, skip])\n",
    "        self.outputs = last_layer(x)\n",
    "        self.model = tf.keras.Model(inputs=self.inputs, outputs=self.outputs)\n",
    "        return self.model"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-05-22T12:12:59.760837Z",
     "iopub.execute_input": "2021-05-22T12:12:59.761272Z",
     "iopub.status.idle": "2021-05-22T12:12:59.773711Z",
     "shell.execute_reply.started": "2021-05-22T12:12:59.761223Z",
     "shell.execute_reply": "2021-05-22T12:12:59.772607Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Resize layer initialization\n",
    "`tf.image.resize` normally returns a tensor which cannot be concatenated with another layer. That's where my custom `tf.keras.layers.ResizeLayer` comes in handy."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class ResizeLayer(layers.Layer):\n",
    "    def __init__(self, target_size, method='bicubic', **kwargs):\n",
    "        super(ResizeLayer, self).__init__(**kwargs)\n",
    "        self.target_size = target_size\n",
    "        self.method = method\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.image.resize(inputs, self.target_size, method=self.method)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Discriminator Model initialization\n",
    "Discriminator model is a PatchGAN. Its output is a 3D vector referring to similarity between patches of input and target images.\n",
    "It determines whether image is real (actual bicubic upscaling) or generated (by our model)."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class Discriminator:\n",
    "    def __init__(self):\n",
    "        self.initializer = tf.random_normal_initializer(0., 0.02)\n",
    "        self.inputs = layers.Input(shape=[256, 256, 3], name='input_image')\n",
    "        self.target = layers.Input(shape=[512, 512, 3], name='target_image')\n",
    "        self.outputs = None\n",
    "        self.model = None\n",
    "\n",
    "    def build(self):\n",
    "        inp_resized = ResizeLayer(target_size=(512, 512), method='bicubic')(self.inputs)\n",
    "        x = layers.concatenate([inp_resized, self.target])  # (bs, 512, 512, 6)\n",
    "        x = downsample(32, 4, instance_norm=False)(x) # (bs, 256, 256, 32)\n",
    "        x = downsample(64, 4)(x)  # (bs, 128, 128, 64)\n",
    "        x = downsample(128, 4)(x)  # (bs, 64, 64, 128)\n",
    "        x = downsample(256, 4)(x)  # (bs, 32, 32, 256)\n",
    "\n",
    "        x = layers.ZeroPadding2D()(x)  # (bs, 34, 34, 256)\n",
    "        x = layers.Conv2D(512, 4, strides=1, kernel_initializer=self.initializer, use_bias=False)(x)  # (bs, 31, 31, 512)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU()(x)\n",
    "        x = layers.ZeroPadding2D()(x)  # (bs, 33, 33, 512)\n",
    "        x = layers.Conv2D(1, 4, strides=1, kernel_initializer=self.initializer)(x)  # (bs, 30, 30, 1)\n",
    "\n",
    "        self.outputs = x\n",
    "        self.model = tf.keras.Model(inputs=[self.inputs, self.target], outputs=self.outputs)\n",
    "        return self.model"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-05-22T12:13:02.737286Z",
     "iopub.execute_input": "2021-05-22T12:13:02.737937Z",
     "iopub.status.idle": "2021-05-22T12:13:02.749260Z",
     "shell.execute_reply.started": "2021-05-22T12:13:02.737899Z",
     "shell.execute_reply": "2021-05-22T12:13:02.748477Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Generator and Discriminator initialization  "
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with tf.device(device):\n",
    "    generator = Generator().build()\n",
    "    discriminator = Discriminator().build()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Visualizing the models\n",
    "Requires `Graphviz` to be installed!\n",
    "> If you want to export the models to _.png_, you can uncomment the second line in cells below."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "keras.utils.plot_model(generator, show_shapes=True, show_layer_names=True)\n",
    "# keras.utils.plot_model(generator, to_file=\"../docs/imgs/patchgan_generator.png\", show_shapes=True, show_layer_names=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "keras.utils.plot_model(discriminator, show_shapes=True, show_layer_names=True)\n",
    "# keras.utils.plot_model(discriminator, to_file=\"../docs/imgs/patchgan_discriminator.png\", show_shapes=True, show_layer_names=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Generator loss function\n",
    "* **L1 loss** -> mean absolute error between generated and target image to make generated images structurally similar to target images\n",
    "* **GAN loss** -> binary cross entropy loss of discriminator's output on generated images and array of ones\n",
    "> total_loss = GAN_loss + (lambda * L1)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "\n",
    "def patch_generator_loss(disc_generated_output, gen_output, target, _lambda: int = 500) -> tf.Tensor:\n",
    "    gan_loss = cross_entropy(tf.ones_like(disc_generated_output), disc_generated_output)\n",
    "    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
    "    return gan_loss + (_lambda * l1_loss)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Discriminator loss function\n",
    "* **Real loss** -> sigmoid cross entropy loss of real image output and array of ones\n",
    "* **Generated loss** -> sigmoid cross entropy loss of generated image output and array of zeros\n",
    "> total_loss = real_loss + gen_loss"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def patch_discriminator_loss(disc_real_output, disc_gen_output) -> tf.Tensor:\n",
    "    real_loss = cross_entropy(tf.ones_like(disc_real_output), disc_real_output)\n",
    "    gen_loss = cross_entropy(tf.zeros_like(disc_gen_output), disc_gen_output)\n",
    "    return real_loss + gen_loss"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-05-22T12:13:03.074023Z",
     "iopub.execute_input": "2021-05-22T12:13:03.074285Z",
     "iopub.status.idle": "2021-05-22T12:13:03.082037Z",
     "shell.execute_reply.started": "2021-05-22T12:13:03.074258Z",
     "shell.execute_reply": "2021-05-22T12:13:03.081264Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Average loss function\n",
    "This function calculates the average loss of the given batch"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def avg_loss(loss_tensor: tf.Tensor) -> float:\n",
    "    loss = 0\n",
    "    for loss_list in loss_tensor[0]:\n",
    "        loss += sum(loss_list)\n",
    "    dim = len(loss_tensor[0]) * len(loss_tensor[0][0])\n",
    "    return loss / dim"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Defining optimizers\n",
    "Adam optimizer's adaptive learning rate, efficient handling of noisy and sparse gradients and ease of use make it a great choice for complex and dynamic training process in GANs."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "patch_generator_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5)\n",
    "patch_discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-05-22T12:13:03.084212Z",
     "iopub.execute_input": "2021-05-22T12:13:03.084487Z",
     "iopub.status.idle": "2021-05-22T12:13:03.093052Z",
     "shell.execute_reply.started": "2021-05-22T12:13:03.084463Z",
     "shell.execute_reply": "2021-05-22T12:13:03.092157Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Checkpoint saver\n",
    "Due to long training times, it is useful to save the model checkpoints every 10 epochs in case of sudden power outage or other unexpected events."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "checkpoint_dir = \"./training_checkpoints\"\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=patch_generator_optimizer,\n",
    "                                 discriminator_optimizer=patch_discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Image generation function\n",
    "Main function that will be responsible for generating images based on trained model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def generate_images(model, inp, tar):\n",
    "    inp_normalized = (inp / 127.5) - 1\n",
    "    pred = model(inp_normalized, training=True)\n",
    "    pred = (pred + 1) * 127.5\n",
    "    pred = np.array(pred, dtype='uint8')\n",
    "    pred = np.array(pred).reshape((1, 512, 512, 3))\n",
    "    display_list = [np.array(inp[0], dtype='uint8'), np.array(pred[0], dtype='uint8'), np.array(tar[0], dtype='uint8')]\n",
    "    title_list = ['Input (256x256)', 'Upscaled with PatchGAN', 'Target (bicubic - 512x512)']\n",
    "    plt.figure(figsize=(20, 20))\n",
    "\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i + 1)\n",
    "        plt.title(title_list[i])\n",
    "        plt.imshow(display_list[i])\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.show()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-05-22T12:13:03.094514Z",
     "iopub.execute_input": "2021-05-22T12:13:03.094913Z",
     "iopub.status.idle": "2021-05-22T12:13:03.105704Z",
     "shell.execute_reply.started": "2021-05-22T12:13:03.094878Z",
     "shell.execute_reply": "2021-05-22T12:13:03.104850Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Train functions",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# noinspection PyUnusedLocal\n",
    "@tf.function\n",
    "def train_step(inp, tar, epoch):\n",
    "    with tf.device(device):\n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "            gen_output = generator(inp, training=True)\n",
    "    \n",
    "            disc_real_output = discriminator([inp, tar], training=True)\n",
    "            disc_gen_output = discriminator([inp, gen_output], training=True)\n",
    "    \n",
    "            gen_loss = patch_generator_loss(disc_gen_output, gen_output, tar)\n",
    "            disc_loss = patch_discriminator_loss(disc_real_output, disc_gen_output)\n",
    "    \n",
    "        gen_grads = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "        disc_grads = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    \n",
    "        patch_generator_optimizer.apply_gradients(zip(gen_grads, generator.trainable_variables))\n",
    "        patch_discriminator_optimizer.apply_gradients(zip(disc_grads, discriminator.trainable_variables))\n",
    "        \n",
    "        return tf.reduce_mean(gen_loss), tf.reduce_mean(disc_loss)\n",
    "\n",
    "\n",
    "def fit(inp_train, tar_train, epochs: int, make_checkpoints: bool = False):\n",
    "    with tf.device(device):\n",
    "        for epoch in range(epochs):\n",
    "            for inp, tar in tqdm(zip(inp_train, tar_train)):\n",
    "                gen_loss, disc_loss = train_step(inp, tar, epoch)\n",
    "            print(f\"Epoch {epoch + 1}/{epochs} -> gen_loss={gen_loss:.4f}, disc_loss={disc_loss:.4f}\\n\",\n",
    "                                            file=stderr)  # saving the average loss values every epoch\n",
    "            \n",
    "            if make_checkpoints and (epoch + 1) % 20 == 0:  # checkpoints every 20 epochs\n",
    "                checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "        \n",
    "        if make_checkpoints and epochs % 20 != 0:  # save final state as a checkpoint\n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "        generator.save_weights(\"../models/patch_gan.weights.h5\")  # and export weights"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-05-22T12:13:03.107085Z",
     "iopub.execute_input": "2021-05-22T12:13:03.107624Z",
     "iopub.status.idle": "2021-05-22T12:19:50.780275Z",
     "shell.execute_reply.started": "2021-05-22T12:13:03.107586Z",
     "shell.execute_reply": "2021-05-22T12:19:50.779097Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# generator.save_weights(\"../models/patch_gan.weights.h5\")  # in case export above fails due to for example lack of memory",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training the model\n",
    "I've tried training for many different amount of epochs, but the best results were achieved after exactly 60 epochs.\n",
    "Higher values used to cause overfitting that would then occasionally create really weird artifacts inside of the upscaled images."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_epochs = 60\n",
    "fit(img256_train, img512_train, epochs=train_epochs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Sample upscaling"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for image in img256_test:\n",
    "    generate_images(generator, (image + 1) * 127.5, (image + 1) * 127.5)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ]
}
